# Text Evaluation Service

Servicio de evaluaci√≥n automatizada de textos educativos optimizado para deployment en **RunPod.io** usando FastAPI, vLLM y el modelo `carlosvillu/gemma2-9b-teacher-eval-nota-feedback`.

## üöÄ Quick Start para RunPod.io

### Comando de Deployment (Un solo paso)

```bash
docker run --gpus all -p 8000:8000 text-eval-service
```

Eso es todo. El servicio estar√° disponible en `http://localhost:8000` con:
- ‚úÖ Modelo cargado autom√°ticamente
- ‚úÖ GPU L40 configurada optimizadamente 
- ‚úÖ Variables de entorno preconfiguradas
- ‚úÖ Health check en `/health`

## üìã Requisitos RunPod.io

- **GPU**: NVIDIA L40 (24GB VRAM) recomendada
- **Memoria**: M√≠nimo 20GB VRAM disponible
- **Docker**: Con soporte `--gpus all`
- **Red**: Acceso a Hugging Face para descarga inicial del modelo

## üîß Build del Container

```bash
# Clonar repositorio
git clone https://github.com/carlosvillu/api-texts-evaluation.git
cd api-texts-evaluation

# Build del container
docker build -t text-eval-service .

# Ejecutar con GPU
docker run --gpus all -p 8000:8000 text-eval-service
```

## ‚ö° Configuraci√≥n GPU L40 Optimizada

El servicio incluye configuraci√≥n preoptimizada para GPU L40:

```env
GPU_MEMORY_UTILIZATION=0.90    # 21.6GB de 24GB
BATCH_SIZE=10                  # √ìptimo para L40
MAX_MODEL_LEN=1024            # Balance memoria/velocidad
COMPILATION_LEVEL=2           # Optimizaci√≥n CUDA
ENABLE_PREFIX_CACHING=true    # Cache para prompts similares
```

## üß™ Testing del Servicio

### Health Check R√°pido
```bash
# Verificar que todo funciona
curl http://localhost:8000/health

# Respuesta esperada:
{
  "status": "healthy",
  "model_loaded": true,
  "gpu_available": true,
  "active_jobs": 0
}
```

### Test de Evaluaci√≥n B√°sico
```bash
# Usar cliente de prueba incluido
python scripts/test_client.py --test basic

# O test completo
python scripts/test_client.py --test full
```

### Test Manual con curl
```bash
# 1. Enviar evaluaci√≥n
curl -X POST http://localhost:8000/evaluate \
  -H "Content-Type: application/json" \
  -d '{
    "items": [
      {
        "id_alumno": "TEST_001",
        "curso": "3r ESO",
        "consigna": "Explica la fotos√≠ntesi",
        "respuesta": "La fotos√≠ntesi √©s quan les plantes fan menjar amb la llum del sol."
      }
    ]
  }'

# 2. Recibir job_id y hacer stream
curl http://localhost:8000/stream/{job_id}
```

## üìä API Endpoints

### `POST /evaluate`
Env√≠a batch de textos para evaluaci√≥n as√≠ncrona.

**Request:**
```json
{
  "items": [
    {
      "id_alumno": "A001",
      "curso": "3r ESO", 
      "consigna": "Explica qu√® √©s la fotos√≠ntesi",
      "respuesta": "La fotos√≠ntesi √©s quan les plantes..."
    }
  ]
}
```

**Response:**
```json
{
  "job_id": "uuid-v4",
  "total_items": 1,
  "estimated_time_seconds": 2,
  "status": "queued",
  "stream_url": "/stream/uuid-v4"
}
```

### `GET /stream/{job_id}`
Stream SSE de resultados procesados en tiempo real.

**Event Format:**
```json
{
  "event": "batch_complete",
  "data": {
    "batch_number": 1,
    "results": [
      {
        "id_alumno": "A001",
        "nota": 4,
        "feedback": "Bon treball per a 3r ESO. La respuesta demostra..."
      }
    ],
    "progress": {
      "completed": 1,
      "total": 1,
      "percentage": 100.0,
      "elapsed_seconds": 2.1,
      "estimated_remaining_seconds": 0,
      "avg_time_per_item": 2.1
    }
  }
}
```

### `GET /health`
Health check con estado del modelo y GPU.

## üê≥ Docker Configuration

### Variables de Entorno Soportadas

```bash
# Modelo y GPU
MODEL_NAME=carlosvillu/gemma2-9b-teacher-eval-nota-feedback
GPU_MEMORY_UTILIZATION=0.90
MAX_MODEL_LEN=1024
TENSOR_PARALLEL_SIZE=1
DTYPE=bfloat16

# API Configuration  
HOST=0.0.0.0
PORT=8000
BATCH_SIZE=10
JOB_TTL_SECONDS=3600
MAX_CONCURRENT_JOBS=5

# Performance Tuning
ENABLE_PREFIX_CACHING=true
BLOCK_SIZE=16
MAX_NUM_SEQS=128
COMPILATION_LEVEL=2
VLLM_USE_PRECOMPILED=1

# Service Configuration
CLEANUP_INTERVAL_SECONDS=300
TIME_ESTIMATION_PER_ITEM=2
SSE_POLLING_INTERVAL=1

# RunPod Specific
RUNPOD_GPU_TYPE=L40
RUNPOD_MEMORY_LIMIT=24GB
```

### Dockerfile Optimizado

El `Dockerfile` incluido est√° preoptimizado para RunPod.io:

- ‚úÖ CUDA 12.1 runtime
- ‚úÖ Python 3.11 
- ‚úÖ Dependencias preinstaladas (vLLM, FastAPI, etc.)
- ‚úÖ Cach√© de modelos configurado
- ‚úÖ Health check integrado
- ‚úÖ Script de inicio optimizado

## üìà M√©tricas de Performance

### Objetivos en GPU L40
- **Throughput**: ‚â•5 inferencias/segundo
- **Latencia P95**: <3 segundos por inferencia
- **Memory Footprint**: <22GB para batches de 2000 textos
- **GPU Utilization**: >80% durante procesamiento
- **SSE Latency**: <100ms por evento

### Monitoreo
```bash
# Memoria GPU en tiempo real
watch -n 1 nvidia-smi

# Estado del servicio
curl -s http://localhost:8000/health | jq

# Jobs activos
curl -s http://localhost:8000/health | jq '.active_jobs'
```

## üîç Troubleshooting RunPod.io

### GPU no detectada
```bash
# Verificar NVIDIA runtime
nvidia-smi

# Verificar Docker GPU support
docker run --rm --gpus all nvidia/cuda:12.1-base nvidia-smi
```

### Modelo no se carga
```bash
# Verificar logs del container
docker logs text-eval-service

# Verificar conectividad Hugging Face
curl -I https://huggingface.co

# Limpiar cach√© y reintentar
docker run --gpus all -p 8000:8000 \
  -e HF_HUB_CACHE=/tmp/cache \
  text-eval-service
```

### Out of Memory
```bash
# Reducir GPU memory utilization
docker run --gpus all -p 8000:8000 \
  -e GPU_MEMORY_UTILIZATION=0.80 \
  -e BATCH_SIZE=5 \
  text-eval-service
```

### Puertos ocupados
```bash
# Usar puerto alternativo
docker run --gpus all -p 8080:8000 text-eval-service

# O especificar puerto interno
docker run --gpus all -p 8080:8080 \
  -e PORT=8080 \
  text-eval-service
```

## üìÅ Estructura del Proyecto

```
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI app principal
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Settings con Pydantic
‚îÇ   ‚îú‚îÄ‚îÄ models.py            # Esquemas de datos
‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py    # POST /evaluate, GET /stream/{job_id}
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ health.py        # GET /health
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inference.py     # Motor vLLM (migrado de notebook)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ state.py         # Gesti√≥n de jobs en memoria
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ timing.py        # Calculadora de progreso
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ start.sh            # Script inicio optimizado RunPod.io
‚îÇ   ‚îî‚îÄ‚îÄ test_client.py      # Cliente prueba con SSE
‚îú‚îÄ‚îÄ Dockerfile              # Optimizado para GPU L40
‚îú‚îÄ‚îÄ docker-compose.yml      # Para desarrollo local
‚îú‚îÄ‚îÄ requirements.txt        # Dependencias Python
‚îú‚îÄ‚îÄ .env.example           # Variables de entorno
‚îú‚îÄ‚îÄ prd.md                 # Product Requirements Document
‚îî‚îÄ‚îÄ vllm_inference_model.ipynb  # Notebook original
```

## üéØ Flujo de Trabajo T√≠pico

1. **Cliente** env√≠a `POST /evaluate` con batch de textos
2. **API** crea `job_id` y responde inmediatamente  
3. **InferenceEngine** procesa en background por batches de 10
4. **Cliente** conecta a `/stream/{job_id}` para recibir resultados
5. **API** env√≠a eventos SSE cada batch completado
6. **Evento final** "complete" indica terminaci√≥n
7. **StateManager** limpia job despu√©s de TTL (1h)

## üß¨ Modelo de Evaluaci√≥n

### Modelo Utilizado
`carlosvillu/gemma2-9b-teacher-eval-nota-feedback` - Modelo Gemma2 fine-tuned para evaluaci√≥n de textos educativos en catal√°n.

### Formato de Evaluaci√≥n
- **Nota**: Escala 0-5 (0=Molt per sota, 5=Excel¬∑lent)
- **Feedback**: Comentario constructivo en catal√°n adaptado al nivel educativo
- **Contexto**: Considera curso del alumno y complejidad de la consigna

### Ejemplo de Respuesta
```json
{
  "id_alumno": "A001",
  "nota": 4,
  "feedback": "Bon treball per a 3r ESO. La respuesta demostra una comprensi√≥ s√≥lida del concepte de fotos√≠ntesi. Supera les expectatives en explicar la import√†ncia per a la vida. Per millorar, podries afegir m√©s detalls sobre els components necessaris (clorofil¬∑la, ATP, etc.)."
}
```

## üö® Limitaciones v1.0

- Sin autenticaci√≥n/autorizaci√≥n
- Sin persistencia permanente (solo memoria con TTL 1h)  
- Sin gesti√≥n avanzada de errores
- Sin m√©tricas detalladas (Prometheus)
- Una √∫nica versi√≥n del modelo
- Sin tests automatizados
- Servicio interno sin rate limiting

## üìû Soporte

Para issues espec√≠ficos del deployment:
- **RunPod.io**: Verificar configuraci√≥n GPU y logs del container
- **Modelo**: Comprobar acceso a Hugging Face Hub
- **Performance**: Ajustar `GPU_MEMORY_UTILIZATION` y `BATCH_SIZE`

Ver logs detallados:
```bash
docker logs text-eval-service --follow
```

---

**¬øTodo listo para evaluar textos?** 
```bash
docker run --gpus all -p 8000:8000 text-eval-service
```