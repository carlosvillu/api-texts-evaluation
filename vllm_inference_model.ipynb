{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311bc90f-8889-4c80-9ed2-cf7df1c868a8",
   "metadata": {},
   "outputs": [],
   "source": [
    " #!pip install vllm\n",
    " #!pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/\n",
    " #!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da12b0-696e-43b5-94e8-94542d64f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Eliminar cach√© anterior (¬°CUIDADO! esto borra modelos descargados)\n",
    "cache_dir = \"/root/.cache/huggingface\"\n",
    "if os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)\n",
    "    print(f\"Cach√© eliminado: {cache_dir}\")\n",
    "\n",
    "# Configurar ANTES de cualquier importaci√≥n\n",
    "os.environ['HF_HOME'] = '/workspace'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/transformers'\n",
    "os.environ['HF_HUB_CACHE'] = '/workspace/hub'\n",
    "\n",
    "# Crear directorios\n",
    "os.makedirs('/workspace/transformers', exist_ok=True)\n",
    "os.makedirs('/workspace/hub', exist_ok=True)\n",
    "\n",
    "# REINICIA EL KERNEL del notebook despu√©s de esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8504f6eb-8a91-44cb-97db-3f378031792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Inicializar el modelo\n",
    "llm = LLM(\n",
    "    model=\"carlosvillu/gemma2-9b-teacher-eval-nota-feedback\",\n",
    "    gpu_memory_utilization=0.95,  # Usar m√°s memoria disponible\n",
    "    max_model_len=1024,  # Reducir un poco desde 8192\n",
    "    tensor_parallel_size=1,\n",
    "    dtype=\"bfloat16\"  # Usar float16 para eficiencia\n",
    ")\n",
    "\n",
    "# Configurar par√°metros de generaci√≥n\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    max_tokens=50\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0492c002-c222-4ddf-971e-8591aa56586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Optimizaciones de entorno para L4 (sin FlashInfer)\n",
    "os.environ[\"VLLM_USE_PRECOMPILED\"] = \"1\"\n",
    "\n",
    "# Limpiar cach√© GPU antes de cargar\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Configuraci√≥n optimizada para NVIDIA L4\n",
    "llm = LLM(\n",
    "    model=\"carlosvillu/gemma2-9b-teacher-eval-nota-feedback\",\n",
    "    gpu_memory_utilization=0.90,  # Aumentado para asegurar KV cache\n",
    "    max_model_len=1024,           # Reducido para liberar memoria\n",
    "    tensor_parallel_size=1,\n",
    "    dtype=\"bfloat16\",\n",
    "    enable_prefix_caching=True,\n",
    "    disable_log_stats=True,       # Menos logging para mejor rendimiento\n",
    "    block_size=16,               # Optimizado para L4\n",
    "    max_num_seqs=128,            # M√°s secuencias concurrentes\n",
    "    compilation_config={\n",
    "        \"level\": 2,              # Compilaci√≥n m√°s r√°pida que level 3\n",
    "        \"use_inductor\": True,\n",
    "        \"use_cudagraph\": True,\n",
    "        \"cache_dir\": \"/tmp/vllm_cache\"  # Reutilizar compilaciones\n",
    "    }\n",
    ")\n",
    "\n",
    "# Par√°metros de generaci√≥n optimizados\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    max_tokens=200,              # M√°s tokens de salida\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Modelo cargado y optimizado para NVIDIA L4\")\n",
    "print(f\"üìä Memoria GPU utilizada: ~17GB\")\n",
    "print(f\"üöÄ Listo para inferencia r√°pida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f1e046-aa4d-445b-bd00-51a8935a9558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompts_correctly(examples):\n",
    "    \"\"\"\n",
    "    Formato mejorado: nota √∫nica con feedback contextual\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    # Procesar cada ejemplo en el batch\n",
    "    for i in range(len(examples[\"pregunta\"])):\n",
    "        # Obtener evaluaciones\n",
    "        evaluaciones = [\n",
    "            examples.get(\"evaluacion_1\", [None])[i],\n",
    "            examples.get(\"evaluacion_2\", [None])[i], \n",
    "            examples.get(\"evaluacion_3\", [None])[i]\n",
    "        ]\n",
    "        evaluaciones = [e for e in evaluaciones if e is not None]\n",
    "        \n",
    "        if not evaluaciones:\n",
    "            continue\n",
    "        \n",
    "        # Calcular nota final (mediana es m√°s robusta que promedio)\n",
    "        nota_final = round(statistics.median(evaluaciones))\n",
    "        curso = examples[\"curso\"][i]\n",
    "        \n",
    "        # Generar feedback basado en nota Y nivel educativo\n",
    "        if nota_final == 0:\n",
    "            feedback = f\"Molt per sota del nivell esperat per a {curso}. Cal reescriure completament amb m√©s contingut i estructura.\"\n",
    "        elif nota_final == 1:\n",
    "            feedback = f\"Per sota del nivell de {curso}. Necessita millores significatives en desenvolupament i claredat.\"\n",
    "        elif nota_final == 2:\n",
    "            feedback = f\"Just acceptable per a {curso}. Cal ampliar les idees i millorar l'expressi√≥.\"\n",
    "        elif nota_final == 3:\n",
    "            feedback = f\"Adequat pel nivell de {curso}. Compleix les expectatives b√†siques del curs.\"\n",
    "        elif nota_final == 4:\n",
    "            feedback = f\"Bon treball per a {curso}. Supera les expectatives amb bon desenvolupament.\"\n",
    "        else:  # nota_final == 5\n",
    "            feedback = f\"Excel¬∑lent per a {curso}. Molt per sobre del nivell esperat, amb idees complexes ben expressades.\"\n",
    "        \n",
    "        # Instrucciones claras para el modelo\n",
    "        instruction = \"\"\"Ets una professora experimentada avaluant textos d'estudiants catalans.\n",
    "Has d'avaluar amb una nota de 0 a 5 i proporcionar feedback constructiu.\n",
    "\n",
    "Escala d'avaluaci√≥:\n",
    "0 = Molt per sota del nivell\n",
    "1 = Per sota del nivell \n",
    "2 = Just acceptable\n",
    "3 = Nivell esperat\n",
    "4 = Per sobre del nivell\n",
    "5 = Excel¬∑lent\n",
    "\n",
    "Respon NOM√âS amb JSON: {\"nota\": X, \"feedback\": \"...\"}\"\"\"\n",
    "        \n",
    "        # Contenido del usuario\n",
    "        user_content = f\"\"\"{instruction}\n",
    "\n",
    "Alumne de {curso} respon a \"{examples[\"pregunta\"][i]}\":\n",
    "{examples[\"respuesta\"][i]}\"\"\"\n",
    "        \n",
    "        # Respuesta esperada del asistente\n",
    "        assistant_response = f'{{\"nota\": {nota_final}, \"feedback\": \"{feedback}\"}}'\n",
    "        \n",
    "        # Formato Gemma correcto\n",
    "        text = f\"\"\"<start_of_turn>user\n",
    "{user_content}<end_of_turn>\n",
    "<start_of_turn>model\"\"\"\n",
    "        \n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40c52a-395f-4132-9d27-4115a846f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4: Preparaci√≥n del dataset - FORMATO CORRECTO PARA GEMMA\n",
    "from datasets import load_dataset\n",
    "import statistics\n",
    "\n",
    "\n",
    "dataset_train = load_dataset(\"carlosvillu/training-texts\", split=\"train\")\n",
    "dataset_test = load_dataset(\"carlosvillu/training-texts\", split=\"test\")\n",
    "\n",
    "# Aplicar formato\n",
    "dataset_train = dataset_train.map(\n",
    "    format_prompts_correctly,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_train.column_names\n",
    ")\n",
    "\n",
    "dataset_test = dataset_test.map(\n",
    "    format_prompts_correctly,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_test.column_names\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset formateado: {len(dataset_train)} train, {len(dataset_test)} test\")\n",
    "print(f\"Ejemplo de formato:\\n{dataset_train[0]['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148995a4-66d9-48e7-8d84-de024cb9a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer inferencia\n",
    "prompts = dataset_test['text']\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Mostrar resultados\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
